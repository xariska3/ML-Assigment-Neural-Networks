{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8f89e3-3cfd-4202-93d5-ddfbcf1e6add",
   "metadata": {},
   "source": [
    "<img src=\"CBS logo.png\" alt=\"University Logo\" style=\"float: left; margin-right: 10px; width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc32d5-03a9-4f9d-8221-faf2d6f9d2e6",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">- Machine Learning Assigment 3 -</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46578162-1304-495e-8cc0-cd5d155094d1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4fb863-4236-4e27-ba5c-b8289b22215a",
   "metadata": {},
   "source": [
    "**Students names and numbers:**\n",
    "- Charalampos Karatzas, 176175"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec941f3-11c2-45b8-bc8d-2bb8c365ea71",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be6992-72dd-4c36-abd6-267238aa16e7",
   "metadata": {},
   "source": [
    "## Question 1 [Perceptron]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072d90c-777d-4a28-b374-0bfd81f87d45",
   "metadata": {},
   "source": [
    "*Suppose we want to train a perceptron (refer to Figure 1) with weights w1 and w2 and a fixed bias\n",
    "b = −1 Sketch the constraints in weight space corresponding to the following training cases.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb637f05-8d60-49e4-a9a7-fc0411ef94b3",
   "metadata": {},
   "source": [
    "In this task I will use the heaviside stepfunctions: \n",
    "$$\n",
    "\\text{heaviside}(z) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if } z < 0 \\\\\n",
    "1 & \\text{if } z \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$z = \\mathbf{w} \\cdot \\mathbf{x}^\\top + b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7bb2cb-476f-428f-8876-27f0e0688f00",
   "metadata": {},
   "source": [
    "For $t = 1$, $\\mathbf{x} = (1, -1)$:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w} \\mathbf{x}^\\top + b \\ge 0 \\Rightarrow w_1 - w_2 - 1 \\ge 0\n",
    "$$\n",
    "\n",
    "For $t = 0$, $\\mathbf{x} = (-1, -1)$:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w} \\mathbf{x}^\\top + b < 0 \\Rightarrow w_1 - w_2 - 1 < 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31aad5-8995-4b19-971a-3fb996d55c84",
   "metadata": {},
   "source": [
    "<img src=\"Task1.png\" alt=\"Task 1\" style=\"float: left; margin-right: 10px; width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a3caa-bb97-4acf-85bd-3bc1c302a4e3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f700424-20c7-4b3e-b1a6-12de4b0259fa",
   "metadata": {},
   "source": [
    "## Question 2 [Neural Networks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b865a0f-fbe0-468b-9032-95169791a3b3",
   "metadata": {},
   "source": [
    "*The following is a network (refer to Figure 2) of linear neurons, that is, neurons whose output is\n",
    "identical to their net input. The numbers in the circles indicate the outcome of a neuron, and the\n",
    "numbers at connections indicate the value of the corresponding weight.*\n",
    "1. Compute the output of the hidden layer and the output-layer neurons for the given input (0.5,1) and enter those values into the corresponding circles.\n",
    "2. What is the network output for the input (1, 2) (i.e., the left input neuron having the value one\n",
    "and the right one having the value 2)? Do you have to do all the network computations once\n",
    "again to answer this question? Explain why you do or do not have to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919255b4-2dd5-40c6-9a65-b4da23257215",
   "metadata": {},
   "source": [
    "#### 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7893d-9cf0-4333-a1af-ef840af8094f",
   "metadata": {},
   "source": [
    "To compute the output of the hidder layer neurons we use this:\n",
    "$$\n",
    "h_j = \\sum_i w_{j,i} x_i + b_j\n",
    "$$\n",
    "\n",
    "To compute the output of the output-layer neuros we use this:\n",
    "$$\n",
    "y_k = \\sum_j v_{k,j} h_j + c_k\n",
    "$$\n",
    "\n",
    "*** \n",
    "\n",
    "$$\n",
    "h_1 = -1 \\times 0.5 + 1 \\times 0 = -0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = 0.5 \\times 2 + 1 \\times 1 = 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3 = 0.5 \\times (-2) + 1 \\times 1 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_1 = 2 \\times (-0.5) + 2 \\times (-0.5) + 0 \\times 1 = -2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_2 = -2 \\times (-0.5) + 1 \\times 2 + 0 \\times 0.5 = 3\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f313e73-3aaa-42be-8c23-eecfb29c71b6",
   "metadata": {},
   "source": [
    "<img src=\"Task2_1.png\" alt=\"Task2_1.png\" style=\"float: left; margin-right: 10px; width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd4f03-d801-4dd4-ba4b-780319f25777",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc3f57-4138-48e0-99bf-1be74b7ef0dd",
   "metadata": {},
   "source": [
    "#### 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736c1ab-dcb6-4bd7-a328-86a1ba3e07c8",
   "metadata": {},
   "source": [
    "<img src=\"Task2_2.png\" alt=\"Task2_1.png\" style=\"float: left; margin-right: 10px; width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603853e-732e-4b68-b312-f43014f2f806",
   "metadata": {},
   "source": [
    "Its not necessary to compute everything again since it's easy to understand that the input vector is the first one multiplied by 2. So since the network is linear we can go straight away to compute the output which is the output of the first task multiplied by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f0c76-d899-4736-a531-96870b628492",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af85c3c-49e2-4908-99c9-a26f95bd6c08",
   "metadata": {},
   "source": [
    "## Question 3 [Coding]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07654ed0-6bfe-450d-9509-d145c8421910",
   "metadata": {},
   "source": [
    "*In this question, you will use the Fashion MNIST dataset and build a custom model using custom\n",
    "training loops (using a different optimizer with a different learning rate for the upper layers\n",
    "and the lower layers) to tackle image classification in the Fashion MNIST dataset.\n",
    "For example, you can use Sequential model from keras to build your custom model. Please note\n",
    "that Sequential model is suitable for a simple stack of layers with the restriction that each layer can\n",
    "only support exactly one input tensor and one output tensor. Alternatively, you can also use Keras\n",
    "Functional API, which allows you to create models that are more flexible than the models created\n",
    "using Sequential model. Some hints are as follows:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ffc86-c3e3-4d3c-a403-479470fd5d77",
   "metadata": {},
   "source": [
    "1. Only use five epochs and 32 as batch size.\n",
    "2. Only use softmax and ReLU activation functions.\n",
    "3. Use SGD as the lower optimizer with the learning rate of 1e-4 and Nadam as upper optimizer\n",
    "with a learning rate as 1e-3.\n",
    "4. Use Nadam optimizer from Keras and also use sparse categorical cross entropy as a loss function.\n",
    "5. Display the mean training loss and the mean accuracy over each epoch (updated at each itera-\n",
    "tion). Also display, validation loss, and accuracy at the end of each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2490f-e2c0-45d3-8c57-f40b96bd628d",
   "metadata": {},
   "source": [
    "### Import the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f20980-7916-4ca3-8682-aac576e6f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import os #To hide out some info tensorflow gives for my pc\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# to make this notebook ’s output stable across runs\n",
    "np.random.seed (42)\n",
    "tf.random.set_seed (42)\n",
    "\n",
    "#Load the dataset\n",
    "( X_train_full , y_train_full ) , ( X_test , y_test ) = keras.datasets.fashion_mnist.load_data ()\n",
    "\n",
    "#Normalize the values to [0,1]\n",
    "X_train_full = X_train_full.astype( np . float32 ) / 255.\n",
    "X_valid , X_train = X_train_full [:5000] , X_train_full [5000:]\n",
    "y_valid , y_train = y_train_full [:5000] , y_train_full [5000:]\n",
    "X_test = X_test.astype(np.float32 ) / 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef264038-ea29-4d9d-9c97-65ce8a163317",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523cca0-be67-4820-97de-99e5224a9295",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5ad22-46bc-449b-bf89-b7180314d798",
   "metadata": {},
   "source": [
    "**We are going to build a simple Sequential model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683d542-57f2-4f98-a635-c6978277a2fd",
   "metadata": {},
   "source": [
    "- A flatten layer to convert the 28x28 images into a 1D vector\n",
    "- A dense hidden layer with 128 neurons and ReLu activation ( for the lower part )\n",
    "- An ouutput dense layer with 10 neurons and softmax activation ( for the upper layer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7b9001-33cf-4d6c-8594-136029e1ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to use as recommended the Keras Sequential API\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, activation = 'relu'), #For the lower layer\n",
    "    keras.layers.Dense(10,activation = 'softmax') #For the upper layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bbca6-1e78-443a-9c12-98c4600a5cb9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919677f5-96ee-4431-8593-e4b5872a77b8",
   "metadata": {},
   "source": [
    "### Defining Loss Function and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85976b58-32aa-4040-845b-b30e5b2b440b",
   "metadata": {},
   "source": [
    "- Use sparse categorical cross entropy as the loss function\n",
    "- Define metrics to monitor both training and validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b677d41-ec38-4a6a-b322-ee4e0c149406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "loss_fun = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "#Defining training metrics\n",
    "train_loss_metric = keras.metrics.Mean()\n",
    "train_accuracy_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "#Define validation metrics\n",
    "val_loss_metric = keras.metrics.Mean()\n",
    "val_accuracy_metric = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2060ec-4be4-48d9-b864-588b66625909",
   "metadata": {},
   "source": [
    "### Optimizers and Variable Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576f007-082d-4a88-910e-cef29e831442",
   "metadata": {},
   "source": [
    "- SGD with learning rate 1e-4 for the lower layer\n",
    "- Nadam with learning rate 1e-3 for the upper layer\n",
    "- Partition the model's trainable variables into two groups based on layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "202e1ce9-8bca-46d9-91eb-2fc932d86f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create optimizers with the specified learning rates \n",
    "optimizer_lower = keras.optimizers.legacy.SGD(learning_rate = 1e-4)\n",
    "optimizer_upper = keras.optimizers.legacy.Nadam(learning_rate = 1e-3) #legacy because the simple optimizers.SGD said it's too slow for my M2 chip and it reccomended legacy.SGD\n",
    "\n",
    "#Partition variables\n",
    "lower_vars = model.layers[1].trainable_variables\n",
    "upper_vars = model.layers[2].trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551e4eb-0d90-45ab-8c57-4833a4f50ea1",
   "metadata": {},
   "source": [
    "## We made two Custom Functions. The second is based on the book Hands-on Machine Learning with Scikit-Learn,Keras & TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f4c97-a9be-4ef6-b7fe-82cb6308d957",
   "metadata": {},
   "source": [
    "### Custom Training Loop #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba864a-f189-4c65-af4a-c34a5673ea5c",
   "metadata": {},
   "source": [
    "- Create a Dataset for training and validation with a batch size of 32\n",
    "- For each taining batch:\n",
    "    - Use tf.GradientTape to compute loss and gradients\n",
    "    - Split the gradients into two groups(for lower and upper layers)\n",
    "    - Apply the corresponding optimizer update for each group\n",
    "    - Update training metrics and print running resukts every 100 steps\n",
    "- After each epoch, evaluate the model on the validation set and print the validation loss and accuarcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edd8e374-55fe-4e83-a704-f5e1427500ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Step 0: Mean loss = 0.4263, Accuracy = 0.7929\n",
      "Step 100: Mean loss = 0.5795, Accuracy = 0.7929\n",
      "Step 200: Mean loss = 0.5699, Accuracy = 0.7931\n",
      "Step 300: Mean loss = 0.5702, Accuracy = 0.7934\n",
      "Step 400: Mean loss = 0.5652, Accuracy = 0.7947\n",
      "Step 500: Mean loss = 0.5639, Accuracy = 0.7949\n",
      "Step 600: Mean loss = 0.5641, Accuracy = 0.7952\n",
      "Step 700: Mean loss = 0.5645, Accuracy = 0.7960\n",
      "Step 800: Mean loss = 0.5613, Accuracy = 0.7970\n",
      "Step 900: Mean loss = 0.5613, Accuracy = 0.7974\n",
      "Step 1000: Mean loss = 0.5598, Accuracy = 0.7980\n",
      "Step 1100: Mean loss = 0.5578, Accuracy = 0.7986\n",
      "Step 1200: Mean loss = 0.5560, Accuracy = 0.7990\n",
      "Step 1300: Mean loss = 0.5542, Accuracy = 0.7997\n",
      "Step 1400: Mean loss = 0.5527, Accuracy = 0.8000\n",
      "Step 1500: Mean loss = 0.5517, Accuracy = 0.8003\n",
      "Step 1600: Mean loss = 0.5526, Accuracy = 0.8002\n",
      "Step 1700: Mean loss = 0.5513, Accuracy = 0.8008\n",
      "Epoch 1: Training loss = 0.5512, Training accuracy = 0.8009\n",
      "Epoch 1: Validation loss = 0.5152, Validation accuracy = 0.8214\n",
      "\n",
      "Epoch 2/5\n",
      "Step 0: Mean loss = 0.6839, Accuracy = 0.8009\n",
      "Step 100: Mean loss = 0.5331, Accuracy = 0.8012\n",
      "Step 200: Mean loss = 0.5261, Accuracy = 0.8018\n",
      "Step 300: Mean loss = 0.5283, Accuracy = 0.8020\n",
      "Step 400: Mean loss = 0.5330, Accuracy = 0.8020\n",
      "Step 500: Mean loss = 0.5354, Accuracy = 0.8023\n",
      "Step 600: Mean loss = 0.5340, Accuracy = 0.8024\n",
      "Step 700: Mean loss = 0.5318, Accuracy = 0.8029\n",
      "Step 800: Mean loss = 0.5321, Accuracy = 0.8032\n",
      "Step 900: Mean loss = 0.5303, Accuracy = 0.8038\n",
      "Step 1000: Mean loss = 0.5289, Accuracy = 0.8042\n",
      "Step 1100: Mean loss = 0.5264, Accuracy = 0.8047\n",
      "Step 1200: Mean loss = 0.5248, Accuracy = 0.8051\n",
      "Step 1300: Mean loss = 0.5263, Accuracy = 0.8052\n",
      "Step 1400: Mean loss = 0.5257, Accuracy = 0.8055\n",
      "Step 1500: Mean loss = 0.5257, Accuracy = 0.8056\n",
      "Step 1600: Mean loss = 0.5245, Accuracy = 0.8059\n",
      "Step 1700: Mean loss = 0.5221, Accuracy = 0.8064\n",
      "Epoch 2: Training loss = 0.5221, Training accuracy = 0.8064\n",
      "Epoch 2: Validation loss = 0.4948, Validation accuracy = 0.8290\n",
      "\n",
      "Epoch 3/5\n",
      "Step 0: Mean loss = 0.3818, Accuracy = 0.8065\n",
      "Step 100: Mean loss = 0.5126, Accuracy = 0.8066\n",
      "Step 200: Mean loss = 0.5117, Accuracy = 0.8069\n",
      "Step 300: Mean loss = 0.5205, Accuracy = 0.8069\n",
      "Step 400: Mean loss = 0.5197, Accuracy = 0.8072\n",
      "Step 500: Mean loss = 0.5167, Accuracy = 0.8075\n",
      "Step 600: Mean loss = 0.5156, Accuracy = 0.8078\n",
      "Step 700: Mean loss = 0.5152, Accuracy = 0.8080\n",
      "Step 800: Mean loss = 0.5156, Accuracy = 0.8081\n",
      "Step 900: Mean loss = 0.5115, Accuracy = 0.8085\n",
      "Step 1000: Mean loss = 0.5106, Accuracy = 0.8088\n",
      "Step 1100: Mean loss = 0.5097, Accuracy = 0.8090\n",
      "Step 1200: Mean loss = 0.5061, Accuracy = 0.8094\n",
      "Step 1300: Mean loss = 0.5043, Accuracy = 0.8097\n",
      "Step 1400: Mean loss = 0.5043, Accuracy = 0.8100\n",
      "Step 1500: Mean loss = 0.5034, Accuracy = 0.8102\n",
      "Step 1600: Mean loss = 0.5029, Accuracy = 0.8105\n",
      "Step 1700: Mean loss = 0.5022, Accuracy = 0.8108\n",
      "Epoch 3: Training loss = 0.5032, Training accuracy = 0.8107\n",
      "Epoch 3: Validation loss = 0.4818, Validation accuracy = 0.8344\n",
      "\n",
      "Epoch 4/5\n",
      "Step 0: Mean loss = 0.5711, Accuracy = 0.8108\n",
      "Step 100: Mean loss = 0.5145, Accuracy = 0.8109\n",
      "Step 200: Mean loss = 0.5046, Accuracy = 0.8111\n",
      "Step 300: Mean loss = 0.5049, Accuracy = 0.8113\n",
      "Step 400: Mean loss = 0.5006, Accuracy = 0.8115\n",
      "Step 500: Mean loss = 0.4991, Accuracy = 0.8117\n",
      "Step 600: Mean loss = 0.4984, Accuracy = 0.8120\n",
      "Step 700: Mean loss = 0.4959, Accuracy = 0.8122\n",
      "Step 800: Mean loss = 0.4961, Accuracy = 0.8124\n",
      "Step 900: Mean loss = 0.4930, Accuracy = 0.8127\n",
      "Step 1000: Mean loss = 0.4910, Accuracy = 0.8130\n",
      "Step 1100: Mean loss = 0.4920, Accuracy = 0.8131\n",
      "Step 1200: Mean loss = 0.4926, Accuracy = 0.8132\n",
      "Step 1300: Mean loss = 0.4910, Accuracy = 0.8135\n",
      "Step 1400: Mean loss = 0.4919, Accuracy = 0.8136\n",
      "Step 1500: Mean loss = 0.4908, Accuracy = 0.8139\n",
      "Step 1600: Mean loss = 0.4901, Accuracy = 0.8142\n",
      "Step 1700: Mean loss = 0.4892, Accuracy = 0.8144\n",
      "Epoch 4: Training loss = 0.4892, Training accuracy = 0.8145\n",
      "Epoch 4: Validation loss = 0.4706, Validation accuracy = 0.8382\n",
      "\n",
      "Epoch 5/5\n",
      "Step 0: Mean loss = 0.3345, Accuracy = 0.8145\n",
      "Step 100: Mean loss = 0.4814, Accuracy = 0.8146\n",
      "Step 200: Mean loss = 0.4888, Accuracy = 0.8148\n",
      "Step 300: Mean loss = 0.4926, Accuracy = 0.8149\n",
      "Step 400: Mean loss = 0.4851, Accuracy = 0.8152\n",
      "Step 500: Mean loss = 0.4838, Accuracy = 0.8153\n",
      "Step 600: Mean loss = 0.4829, Accuracy = 0.8155\n",
      "Step 700: Mean loss = 0.4829, Accuracy = 0.8157\n",
      "Step 800: Mean loss = 0.4860, Accuracy = 0.8158\n",
      "Step 900: Mean loss = 0.4835, Accuracy = 0.8161\n",
      "Step 1000: Mean loss = 0.4817, Accuracy = 0.8163\n",
      "Step 1100: Mean loss = 0.4804, Accuracy = 0.8165\n",
      "Step 1200: Mean loss = 0.4781, Accuracy = 0.8167\n",
      "Step 1300: Mean loss = 0.4783, Accuracy = 0.8168\n",
      "Step 1400: Mean loss = 0.4781, Accuracy = 0.8171\n",
      "Step 1500: Mean loss = 0.4807, Accuracy = 0.8171\n",
      "Step 1600: Mean loss = 0.4794, Accuracy = 0.8173\n",
      "Step 1700: Mean loss = 0.4784, Accuracy = 0.8175\n",
      "Epoch 5: Training loss = 0.4784, Training accuracy = 0.8176\n",
      "Epoch 5: Validation loss = 0.4606, Validation accuracy = 0.8438\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(batch_size)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "#Custom traiing loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    train_loss_metric.reset_states()\n",
    "    train_accuarcy_metric.reset_states()\n",
    "\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True)\n",
    "            loss_value = loss_fun(y_batch, logits)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "        num_lower = len(lower_vars)\n",
    "        grads_lower = grads[:num_lower]\n",
    "        grads_upper = grads[num_lower:]\n",
    "\n",
    "        optimizer_lower.apply_gradients(zip(grads_lower, lower_vars))\n",
    "        optimizer_upper.apply_gradients(zip(grads_upper, upper_vars))\n",
    "        \n",
    "    \n",
    "        train_loss_metric.update_state(loss_value)\n",
    "        train_accuracy_metric.update_state(y_batch, logits)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}: Mean loss = {train_loss_metric.result().numpy():.4f}, Accuracy = {train_accuracy_metric.result().numpy():.4f}\")\n",
    "    \n",
    "    val_loss_metric.reset_states()\n",
    "    val_accuracy_metric.reset_states()\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        val_loss = loss_fun(y_batch_val, val_logits)\n",
    "        val_loss_metric.update_state(val_loss)\n",
    "        val_accuracy_metric.update_state(y_batch_val, val_logits)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Training loss = {train_loss_metric.result().numpy():.4f}, Training accuracy = {train_accuracy_metric.result().numpy():.4f}\")\n",
    "    print(f\"Epoch {epoch+1}: Validation loss = {val_loss_metric.result().numpy():.4f}, Validation accuracy = {val_accuracy_metric.result().numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd642bd-cf4f-42e3-aacc-50f307451877",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db16bc-e8b5-49bc-84f2-8e062abc0571",
   "metadata": {},
   "source": [
    "### Custom Function #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88e6375-c775-419d-b148-33e126f60522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "    \n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a582479-3ec4-4d65-b4b0-ba0c1d3ad41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55000/55000 - mean: 0.9891 - sparse_categorical_accuracy: 0.7066\n",
      "Validation - Loss: 0.6575, Accuracy: 0.7929\n",
      "Epoch 2/5\n",
      "55000/55000 - mean: 0.6217 - sparse_categorical_accuracy: 0.7901\n",
      "Validation - Loss: 0.5607, Accuracy: 0.8171\n",
      "Epoch 3/5\n",
      "55000/55000 - mean: 0.5541 - sparse_categorical_accuracy: 0.8078\n",
      "Validation - Loss: 0.5246, Accuracy: 0.8241\n",
      "Epoch 4/5\n",
      "55000/55000 - mean: 0.5273 - sparse_categorical_accuracy: 0.8132\n",
      "Validation - Loss: 0.5077, Accuracy: 0.8321\n",
      "Epoch 5/5\n",
      "55000/55000 - mean: 0.5078 - sparse_categorical_accuracy: 0.8217\n",
      "Validation - Loss: 0.4928, Accuracy: 0.8365\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.0\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_train_scaled = X_train\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "lower_vars = model.layers[1].trainable_variables\n",
    "upper_vars = model.layers[2].trainable_variables\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer_upper = keras.optimizers.legacy.Nadam(learning_rate=1e-3)\n",
    "optimizer_lower = keras.optimizers.legacy.SGD(learning_rate=1e-4)\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "val_loss = keras.metrics.Mean(name=\"val_loss\")\n",
    "val_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"val_accuracy\")\n",
    "\n",
    "for epoch in range(1, n_epochs + 1): \n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        num_lower = len(lower_vars)\n",
    "        grads_lower = gradients[:num_lower]\n",
    "        grads_upper = gradients[num_lower:]\n",
    "        optimizer_lower.apply_gradients(zip(grads_lower, lower_vars))\n",
    "        optimizer_upper.apply_gradients(zip(grads_upper, upper_vars))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    n_val_steps = len(X_valid) // batch_size\n",
    "    for v_step in range(1, n_val_steps + 1):\n",
    "        start = (v_step - 1) * batch_size\n",
    "        end = v_step * batch_size\n",
    "        X_val_batch = X_valid[start:end]\n",
    "        y_val_batch = y_valid[start:end]\n",
    "        y_val_pred = model(X_val_batch, training=False)\n",
    "        v_loss = tf.reduce_mean(loss_fn(y_val_batch, y_val_pred))\n",
    "        val_loss(v_loss)\n",
    "        val_accuracy(y_val_batch, y_val_pred)\n",
    "    print(\"Validation - Loss: {:.4f}, Accuracy: {:.4f}\".format(val_loss.result().numpy(), val_accuracy.result().numpy()))\n",
    "    for metric in [mean_loss] + metrics + [val_loss, val_accuracy]:\n",
    "        metric.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafd60d-3830-49b0-bbca-7c1badf8516e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
